\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{changepage}
\usepackage{color}
\usepackage{siunitx}
\usepackage{booktabs}

\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}

\begin{document}

	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % 
		\center % Center everything on the page
		 
		\textsc{\LARGE Indian Institute of Technology, Kanpur}\\[1.5cm] 
		
		
		\HRule \\[0.4cm]
		{ \huge \bfseries Important Concepts}\\[0.4cm] % Title of your document
		\HRule \\[1.5cm]
		 
		\Large \emph{Author:}\\
		Shubham Agrawal\\[3cm] % Your name
		
		
		{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise
		
		\vfill % Fill the rest of the page with whitespace
	
	\end{titlepage}
	
	
	\begin{abstract}
	There are various concepts in probability theory and linear algebra that we keep on forgetting and they keep haunting us throughout our lifetime. I attempt to list and explain a few of them, which I myself need very much. 
	\end{abstract}	
	\section{Eigen vectors and Eigen Values}
	\section{Eigen Value decomposition}
	\section{Singular value decomposition}
	\section{Gram Schmidt Orthogonalization}
	\section{Positive Definite Matrix}
	
		A symmetric (Hermitian) $n \times n$ real ( complex) matrix $M$ is said to be Positive Definite if $z^TMz > 0$ for every non-zero vector $z$. In case of positive semidefinite, $z^TMz \ge 0$. 
		
		\subsection{Properties of Positive definite matrix}
			Let $M$ be an $n \times n$ symmetric matrix.
			\begin{itemize}
				\item It has positive eigen values. Vice-versa, if a matrix has all it's eigenvalues real and positive, the matrix is positive definite. 
				\item For any real invertible matrix $A$, \textbf{$A^{T}A$ is always positive definite}
				\item Positive definite matrix has \textbf{positive determinant}. This means, that PD matrix is always \textbf{nonsingular}
			\end{itemize}
	\section{Covariance Matrix}
	
		Let $\textbf{X} = \{X_1, X_2 \cdots X_n\}$ be a random vector. Then, 
		\begin{equation}
		Cov(\textbf{X}) = [\Sigma]_{n \times n} 
		\end{equation}
		where $\Sigma_{ij} = cov(X_i, X_j) = E((X_i - \mu_i)(X_j - \mu_j))$
		\\
		Also, Let $\textbf{Y} = \{Y_1, Y_2 \cdots Y_m\}$ be another random vector. Then, 
		\begin{equation}
			Cov(\textbf{X, Y}) = E({(\textbf{X} - \mu_\textbf{X})}^T(\textbf{Y} - \mu_\textbf{Y})) = [\Sigma]_{n \times m}
		\end{equation}
	
		\subsection{Covariance}
			Covariance means how two things(random variables $X_1$ and $X_2$) vary with respect to each other. If on increase of $X_1$, $X_2$ increases and on decrease of $X_1$, $X_2$ decreases, we say covariance is positive. If on increase of $X_1$, $X_2$ decreases and vice versa, we say that covariance is negative. Covariance is 0 if no such relation exists. Understanding the magnitude of covariance is difficult. 
			\\
			Mathematically, let $X_1$ and $X_2$ are two real valued random variables. Then Cov($X_1$, $X_2$) = $E[X_1 - \mu_1)\times(X_2-\mu_2)]$
		
		
		\subsection{Properties of Covariance Matrices}
			\begin{itemize}
				\item Positive semidefinite
				\item Symmetric
				\item If ($m = n$)
			\end{itemize}


	\section{Random Variable}
		Random variable is a function that maps outcomes of an event to mathematically convenient form (real numbers). For example: Let our event be "\textit{A coin is tossed 30 times}" and let random variable be \textbf{Number of heads occured while tossing 30 times}
		
		\subsection{Random Vector}
			Random vector are used when you want to view multiple events simultaneously. For example: You want to observe tossing of coin ($X_1$) and rolling of dice ($X_2$) simultaneously. Then, you define random vector $\textbf{X} = \{X_1, X_2\}$. Here $X_1: {-1,1}$ and $X_2: {1,2,3,4,5,6}$. In more machine learning pov, you have random vector as your feature set where each feature is a random variable.

	\section{PCA}
	
\end{document}